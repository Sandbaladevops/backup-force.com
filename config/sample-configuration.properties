# This is example configuration for backup-force.com utility
#
# Create a copy if this file, rename it (e.g. myconf.properties), 
# uncomment relevant lines (i.e. remove # character in front of the line) and set right values,
# and then use in --config parameter. 
# For example if backup-force.com-0.1.jar and myconf.properties
# reside in C:/myfolder then process can be run like this
#	java -jar c:/myfolder/backup-force.com-0.1.jar --config c:/myfolder/myconf.properties
#

# Data will be extracted in the folder defined by outputFolder key

# Unix
#outputFolder=/full/path/to/backup/folder/`date +%Y%m%d-%H%M`

# Windows (using date.exe from UnixUtils: http://sourceforge.net/projects/unxutils/files/)
# assuming date.exe is available in %PATH% or present in the current directory
#outputFolder=c:/full/path/to/backup/folder/`date.exe +%Y%m%d-%H%M`

#lastRunOutputFile=/full/path/to/lastRun.properties


# =========================================
# Connection details
#
# Specify the login credentials for the desired Salesforce organisation
sf.username = my@name.com
sf.password = password-and-token

# uncomment for Production Orgs
#sf.serverurl = https://login.salesforce.com
# uncomment for Sandbox Orgs
#sf.serverurl = https://test.salesforce.com

# The host name of the proxy server, if applicable.
#http.proxyHost=http://myproxy.internal.company.com

# The proxy server port.
#http.proxyPort=8080

#http.proxyUsername=
#http.proxyPassword=

# The name of the Windows domain used for NTLM authentication.
#http.ntlmDomain

# The number of seconds to wait for a connection during API calls.
#http.connectionTimeoutSecs=60

# Specify how many seconds the backup-force.com utility waits to receive a 
# response back from the server before returning an error for the request
#http.readTimeoutSecs=540

# on Object types with very large number of records Bulk API may be quicker than normal mode
# By default Bulk API is disabled because its current SFDC version has many limitations
# sf.useBulkApi = true
# =========================================
# You can specify selected or ALL object types

# selected types
#backup.objects=Account, Contact

# or use * to specify ALL object types
#backup.objects=*

# to exclude certain objects use backup.objects.exclude key
# backup.objects.exclude takes precedence over backup.objects and backup.soql.<type-name>
# backup.objects.exclude is particularly useful with combination backup.objects=*
#backup.objects.exclude=Contact, Opportunity

# =========================================
# You can set a specific per-object query using backup.soql.<ObjectName> key 
# and/or global WHERE (for all other objects) using backup.global.where key

# example 1: select Id and Name field from Account and limit to 10 records
#backup.soql.Account=select Id, Name from Account limit 10

# example 2: Select all fields from Contact where last modified date is this year
#backup.soql.Account=select * from Contact where LastModifiedDate = $Object.LastModifiedDate

# =========================================
# Global where clause is applied to all objects that do not have backup.soql.<ObjectName> property
# be careful because not all objects may support fields you use in global.where

# example 1: extract all records with LastModifiedDate >= 'date of last retrieval', i.e. Incremental extract
#backup.global.where=LastModifiedDate >= $Object.LastModifiedDate

# example 2: extract objects created this year
#backup.global.where=CreatedDate >= THIS_YEAR

# =========================================
# if files (Attachment, Document) should be saved as real files as well as .csv then use backup.extract.file 
# available values: $id, $name, $ext
# e.g. template below will return something like: picture-1234566789012345.jpg
#backup.extract.file=$name-$id$ext

# =========================================
## Post and pre-process scripts
# there are currently 2 hooks which can be configured to call user scripts (any shell script)
# Each script will receive a number of command line parameters as outlined below
#
# Example: assume I have written a script which zips results and deletes original .csv files to conserve drive space
# I could define my hook like this:
#	hook.global.after=/home/user/zip_and_move.sh
# or in windows environment
#	hook.global.after=c:/zip_and_move.cmd
#
# this script will be called when the process ends like this
#	c:/zip_and_move.cmd c:/process/output/folder c:/lastrun.properties

# all hooks also support user defined parameters, which (if provided) will precede dynamic parameters 
# for example following configuration
#	hook.global.after=c:/zip_and_move.cmd
#	hook.global.after.params=param1 param2
# will be called like so
#	c:/zip_and_move.cmd param1 param2 c:/process/output/folder c:/lastrun.properties

# hook.global.before - script run before process starts
# params: 
# - full path to output folder
# - full path to lastRunOutputFile
#hook.global.before=/full/path/to/script

# hook.global.after=script run after process completed
# params: 
# - full path to output folder
# - full path to lastRunOutputFile
#
#hook.global.after=/full/path/to/script

# hook.each.before=script run before each Object process starts
# params: 
# - full path to output folder
# - full path to lastRunOutputFile
# - Object type name
# - full path to <objecttype>.csv which will be created
#
#hook.each.before=/full/path/to/script

# hook.each.after=script run after Each Object process is completed
# params: 
# - full path to output folder
# - full path to lastRunOutputFile
# - Object type name
# - full path to generated <objecttype>.csv
# - number of saved records 
#
#hook.each.after=/full/path/to/script
